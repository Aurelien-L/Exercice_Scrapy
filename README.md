![banner](img/brief_banner.webp)

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)
![Python](https://img.shields.io/badge/Python-3.13-blue)
![Scrapy](https://img.shields.io/badge/Scrapy-2.13-red)
![psycopg2](https://img.shields.io/badge/psycopg2-2.9-yellow)
![FastAPI](https://img.shields.io/badge/FastAPI-0.117-green)



# ğŸ“š Books To Scrape - Projet de Web Scraping

## Description
Ce projet est un systÃ¨me automatisÃ© de collecte de donnÃ©es depuis le site [Books to Scrape](https://books.toscrape.com/). Il extrait les informations dÃ©taillÃ©es des livres, les stocke dans une base de donnÃ©es PostgreSQL et fournit une interface FastAPI pour accÃ©der aux donnÃ©es.

*Projet rÃ©alisÃ© dans le contexte de la formation DÃ©veloppeur IA chez Simplon Hauts-de-France*

## ğŸ› ï¸ Technologies UtilisÃ©es
- **Python 3.13**
- **Scrapy** : Framework de web scraping
- **FastAPI** : Framework API REST
- **PostgreSQL** : Base de donnÃ©es relationnelle
- **SQLAlchemy** : ORM (Object-Relational Mapping)
- **Uvicorn** : Serveur ASGI pour FastAPI

## ğŸ—ï¸ Structure du Projet
```
Exercice_Scrapy/
â”‚
â”œâ”€â”€ api/                    # Code de l'API FastAPI
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ routers/       # Routes API
â”‚       â”œâ”€â”€ crud.py        # OpÃ©rations CRUD
â”‚       â”œâ”€â”€ database.py    # Connexion Ã  la BDD
â”‚       â”œâ”€â”€ main.py        # Porte d'entrÃ©e de l'API
â”‚       â”œâ”€â”€ models.py      # ModÃ¨les SQLAlchemy
â”‚       â””â”€â”€ schemas.py     # SchÃ©mas Pydantic
â”‚
â”œâ”€â”€ scrapy_books/          # Projet Scrapy
â”‚   â””â”€â”€ scrapy_books/
â”‚       â”œâ”€â”€ spiders/       # Spiders Scrapy
â”‚       â”œâ”€â”€ itemloaders.py # Extraction et nettoyage des donnÃ©es
â”‚       â”œâ”€â”€ items.py       # Structure des donnÃ©es
â”‚       â”œâ”€â”€ middlewares.py # Gestion requÃªtes/rÃ©ponses HTTP
â”‚       â”œâ”€â”€ pipelines.py   # Pipelines de traitement
â”‚       â””â”€â”€ settings.py    # ParamÃ¨tres de Scrapy
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ queries.ipynb      # RequÃªtes SQL de tests vers la BDD
â”‚
â”œâ”€â”€ .env                   # Variables d'environnement
â””â”€â”€ run_project.py         # Script d'exÃ©cution du projet
```

## ğŸ“‹ PrÃ©requis
- Python 3.13 ou supÃ©rieur
- PostgreSQL 15 ou supÃ©rieur
- pip (gestionnaire de paquets Python)

## ğŸš€ Installation

1. **Cloner le repository**
```bash
git clone https://github.com/Aurelien-L/Exercice_Scrapy.git
cd Exercice_Scrapy
```

2. **CrÃ©er un environnement virtuel**
```bash
python -m venv .venv
.\.venv\Scripts\activate
```

3. **Installer les dÃ©pendances**
```bash
pip install -r requirements.txt
```

4. **Configurer les variables d'environnement**  
CrÃ©er un fichier `.env` Ã  la racine du projet en suivant la structure du  `.env.example`.  
Utiliser le `START_URL` *all pages* pour extraire la totalitÃ© des livres du site, ou l'autre lien pour faire un test avec les deux derniÃ¨res pages uniquement.

## ğŸ¯ Utilisation

1. **Lancer le projet complet**
```bash
python run_project.py
```
Cette commande va :
- CrÃ©er la base de donnÃ©es si elle n'existe pas
- CrÃ©er les tables nÃ©cessaires
- Lancer le scraping des donnÃ©es
- DÃ©marrer l'API FastAPI

>[!WARNING]
>Assurez-vous d'avoir *PostgreSQL* d'installÃ© sur votre machine (obligatoire).

2. **AccÃ©der Ã  l'API**
- Documentation Swagger UI : http://localhost:8000/docs

## ğŸ“Œ Points d'accÃ¨s API

- `GET /books/...` : Informations sur les livres
- `GET /categories/...` : Informations sur les catÃ©gories
- `GET /stats/...` : Informations statistiques

## ğŸ’¾ SchÃ©ma de la base de donnÃ©es

![schema_bdd](img/schema.png)  

## ğŸ“ Licence
Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ‘¤ Auteur
[ @Aurelien-L ](https://github.com/Aurelien-L)